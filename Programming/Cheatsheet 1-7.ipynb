{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e585006b",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f36feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objs as go\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import functools as ft\n",
    "import statsmodels.api as sm\n",
    "import scipi as sc\n",
    "import statsmodels.stats.api as sms\n",
    "import statsmodels.discrete.discrete_model as smdiscrete\n",
    "import seaborne as sbs\n",
    "import re\n",
    "import plotly.express as px\n",
    "from pandas_datareader import data as pdr\n",
    "from sklearn import linear_model\n",
    "import datetime as dt\n",
    "import yfinance as yf\n",
    "(yf.pdr_override()) # never forget to override!!!\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8a4480",
   "metadata": {},
   "source": [
    "### For Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06612381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Algorithm\n",
    "# with sklearn it's first planning then execution. this is planning!\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Regularization\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Decision Tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# ENSEMBLE\n",
    "\n",
    "# ## Bagging\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "## Boosting\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Support Vector Machine\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# K-Nearest Neighbor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Multi-layer Perceptron (Neural Networks)\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# in linear regression hyperparams are polynomials. high polynomials work good with the dataset but not on real data\n",
    "\n",
    "# # for data split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# for cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# for assessment\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# for Feature Selection\n",
    "from sklearn.feature_selection import chi2, f_regression\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# for time series models\n",
    "import statsmodels.tsa.arima.model as stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# for data preparation and visualisation\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "# for Pre-processing (Feature Engineering)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# assumption checks for Time-Series\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "# for unsupervised learning\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD # singular value decomposition\n",
    "\n",
    "from numpy.linalg import inv, eig, svd\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# for EDA and data transformation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from pandas.plotting import scatter_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486f3b8a",
   "metadata": {},
   "source": [
    "# Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adee4603",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"filename.csv\") #same with excel\n",
    "df.to_csv(\"filename.csv\") # export to csv\n",
    "df_we_want = pd.DataFrame(source file, series, whatever)\n",
    "\n",
    "df.head() #can specify in brackets how many lines want to see\n",
    "df.tail()\n",
    "df.dtypes #gives us datatypes of a df\n",
    "df.describe() #gives us mean, std, min, max\n",
    "df[\"column\"].describe() #gives us count, unique etc of a column\n",
    "df[\"column\"].unique() # gives us unique names of columns\n",
    "print(f\"Example of f-string with {var_1} and {var_2}.\") #f-string\n",
    "\n",
    "df.rename(columns = lambda x: x.strip(), inplace = True) #this one uses lambda to strip off space in each column\n",
    "df1[\"df1 date column\"] = df1[\"the column\"].apply(lambda x: datetime.strptime(x, \"%m/%d/%y\").date()) #use lambda to strip time\n",
    "#lambda is a 'disposable function' we create for a specific task, then it iterates\n",
    "df.map(funct, iterable) #returns a map object(which is an iterator) of the results \n",
    "#after applying the given function to each item of a given iterable (list, tuple etc.)\n",
    "df.append({'col1': col1, 'col2': col2, 'col3': col3 }) # appends the results to the list\n",
    "df.T #transpose\n",
    "\n",
    "df2 = df1[[\"df1_col1\", \"df1_col2\"]] #create a new df2 with some columns from df1\n",
    "df[\"column\"] = df[\"column\"].astype(str) #changes dtype to str/int etc\n",
    "df2[\"new date column name\"] = pd.to_datetime(df1['date column name']) #convert to date format\n",
    "df3 = pd.concat([df1, df2], axis=1) #joins two dfs on either axis 1 or axis 0\n",
    "df2 = df1.groupby(\"column name\").sum() # group by a col and sum results\n",
    "df.sort_values(by = \"column name\", ascending = False) #sort values descending\n",
    "df = df2.loc['column name'] #finds and puts column in new df by name\n",
    "df = df2.iloc['column index'] #finds and puts column in new df by index\n",
    "df.shape #gives us no of rows and cols\n",
    "\n",
    "df2 = pd.DataFrame(df1[\"column whose values we count\"].value_counts()) #count in a new df the unique values\n",
    "df2 = df1[\"column which we count\"].count() #count all the values EXCLUDING NULL AND NAN\n",
    "df[\"column that we check\"].isnull().sum() #check for null and nan\n",
    "df.dropna() #drop nan values, can specify axis and inplace\n",
    "df2 = df1.fillna(0) #fill nan with 0\n",
    "np.sign(sample_array) #check for a sign of every number in array; need for ts momentum\n",
    "\n",
    "x = (np.linspace(0, 10))# returns evenly spaced numbers over a specified interval here(0-10)\n",
    "#By default, num = 50, meaning it will return an array of 50 evenly spaced values between the start and stop values.\n",
    "np.polyval(ols, x) #evaluates the polynom at specific x values\n",
    "np.linalg.lstsq(a, b, rcond=None[0]) #a - predictor matrix. b - dependent matrix, rcond - cutoff ratio (we use 0)\n",
    "# The np.linalg.lstsq() function returns several outputs, including the solution to the least squares problem, \n",
    "# the residuals, the rank of the matrix, and the singular values. \n",
    "#By appending [0] at the end, we're only selecting the solution, which is the coefficients of the OLS regression.\n",
    "linear_model.LogisticRegression(solver = \"lbfgs\", C = 1e7, multi_class = \"auto\", max_iter = 1000)\n",
    "# here,  specifies the algorithm to use in the optimization problem. \n",
    "#\"lbfgs\" stands for Limited-memory Broyden–Fletcher–Goldfarb–Shanno\n",
    "# C represents the inverse of regularization strength. Smaller values specify stronger regularization (smaller value). \n",
    "# Regularization is a technique used to prevent overfitting by penalizing large values of the model parameters\n",
    "# The \"auto\" option means the model will choose the binary approach \n",
    "# if the target is binary (2 classes) or the \"ovr\" (One-vs-Rest) approach if the target is multiclass.\n",
    "# max_iter = 1000 specifies the maximum number of iterations for the solver to converge\n",
    "correlation = df.corr() #for correlation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfc93c8",
   "metadata": {},
   "source": [
    "### Simple example with map, def and lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84a52aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addition(q):\n",
    "     return q + q\n",
    "#We double all numbers using map()\n",
    "numbers = (1, 2, 3, 4)\n",
    "result = map(addition, numbers)\n",
    "print(list(result)\n",
    "     )\n",
    "# or we can use lambdas\n",
    "numbers = (1, 2, 3, 4)\n",
    "result = map(lambda x: x + x, numbers)\n",
    "print(list(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933b4eb6",
   "metadata": {},
   "source": [
    "### Extract data from yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd6451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of tickers we want, create start end, use def\n",
    "\n",
    "start = dt.datetime(2011,8,1)\n",
    "end = dt.datetime(2020,8,1)\n",
    "TICKERS = [\"AAPL\", \"MSFT\", \"NFLX\", \"AMZN\", \"BA\", \"UAL\", \"GS\", \"JPM\"]\n",
    "def extract_multiple_stocks(TICKERS, start, end):\n",
    "    def data(ticker):\n",
    "        return pdr.get_data_yahoo(ticker, start,end)\n",
    "    interim = map(data, TICKERS)\n",
    "    return pd.concat(interim, keys = TICKERS, names = [\"ticker\", \"date\"])\n",
    "eight_stocks = extract_multiple_stocks(TICKERS, start, end)\n",
    "\n",
    "#we have 8 stocks so it's useful to reset index\n",
    "adjusted_closing_prices =(eight_stocks[[\"Adj Close\"]].reset_index())\n",
    "#resetting to implicit index (original index will be in one of the columns - ticker) \n",
    "#it can be accessed by iloc, explicit by .loc\n",
    "#double square bracket is DF, single square is series, so that is why with multiple selection always double brackets!\n",
    "\n",
    "daily_closing_prices_WIDE =(adjusted_closing_prices.pivot(index = \"date\", columns = \"ticker\", values = \"Adj Close\"))\n",
    "# purpose of pivot: df has vertical and horisontal format. index is not part of column\n",
    "# daily_closing_prices_T does it the bad way. pivot allows to choose indice, columns etc\n",
    "\n",
    "(daily_closing_prices_WIDE[\"JPM\"].plot(figsize = [16, 6])) #to plot a particular stock from df\n",
    "\n",
    "f, ax =(plt.subplots(figsize = [16, 6]))\n",
    "(plt.bar(daily_volumes_AMZN.index, daily_volumes_AMZN)) # to plot a bar of daily volumes\n",
    "\n",
    "dpc_method_shift = (daily_closing_prices_WIDE / daily_closing_prices_WIDE.shift(1) - 1)\n",
    "# to calculate % of daily change\n",
    "dpc_method_pct_change = (daily_closing_prices_WIDE.pct_change())\n",
    "dpc_method_shift.iloc[ : , ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6dbdf1",
   "metadata": {},
   "source": [
    "### Convenient extraction of multiple stocks in a pivot taking only Adj Close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2904434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sp(stocks, start, end):\n",
    "    def data(ticker):\n",
    "        return(pdr.get_data_yahoo(ticker,\n",
    "                                  start = start,\n",
    "                                  end = end)\n",
    "              )\n",
    "    FAANG_Stock = map(data, stocks)\n",
    "    return(pd.concat(FAANG_Stock,\n",
    "                     keys = stocks,\n",
    "                     names = [\"Company\", \"Date\"]\n",
    "                    )\n",
    "          )\n",
    "FAANG =\\\n",
    "    extract_sp(stocks,\n",
    "               datetime.datetime(2014, 9, 1),\n",
    "               datetime.datetime(2021, 8, 31)\n",
    "              )\n",
    "Daily_Closing_Price =\\\n",
    "(\n",
    "FAANG[[\"Adj Close\"]]\n",
    "    .reset_index()\n",
    "    .pivot(index = \"Date\",\n",
    "           columns = \"Company\",\n",
    "           values = \"Adj Close\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa6c8ea",
   "metadata": {},
   "source": [
    "### Candlesticks from plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0926c6d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sp500' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sp500 \u001b[38;5;241m=\u001b[39m(sp500\u001b[38;5;241m.\u001b[39mreset_index())\n\u001b[0;32m      2\u001b[0m CLOSE_sp500 \u001b[38;5;241m=\u001b[39m(go\u001b[38;5;241m.\u001b[39mScatter(x \u001b[38;5;241m=\u001b[39m sp500\u001b[38;5;241m.\u001b[39mDate, y \u001b[38;5;241m=\u001b[39m sp500\u001b[38;5;241m.\u001b[39mClose))\n\u001b[0;32m      3\u001b[0m go\u001b[38;5;241m.\u001b[39mFigure(CLOSE_sp500) \u001b[38;5;66;03m#normal graph\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sp500' is not defined"
     ]
    }
   ],
   "source": [
    "sp500 =(sp500.reset_index())\n",
    "CLOSE_sp500 =(go.Scatter(x = sp500.Date, y = sp500.Close))\n",
    "go.Figure(CLOSE_sp500) #normal graph\n",
    "# INTERACTIVE Candlestick chart\n",
    "\n",
    "candlestick =[go.Candlestick(x = sp500.Date, open = sp500.Open, high = sp500.High, low = sp500.Low, close = sp500.Close)]\n",
    "go.Figure(candlestick)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e278f5",
   "metadata": {},
   "source": [
    "### Cumulative return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec273e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_daily_return =((1 + dpc_percent_change).cumprod())\n",
    "\n",
    "#visualize it on histo\n",
    "tickername_pc = dpc_percent_change[\"tickername\"]\n",
    "dpc_method_pct_change[\"GS\"].hist(bins = 50)\n",
    "tickername_pc.hist(bins = 50)\n",
    "# to describe the percentiles\n",
    "(tickername_pc.describe(percentiles = [0.025, 0.10, 0.5, 0.90, 0.975]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28715167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison between % change of different stocks\n",
    "\n",
    "def corr_plot_for_you(data,\n",
    "                      x_stock_name,\n",
    "                      y_stock_name,\n",
    "                      xlim = None,\n",
    "                      ylim = None):\n",
    "    \n",
    "    f = plt.figure(figsize = [4, 4]\n",
    "                  )\n",
    "    ax = f.add_subplot(111)\n",
    "    \n",
    "    ax.scatter(data[x_stock_name],\n",
    "               data[y_stock_name],\n",
    "               alpha = 0.20)\n",
    "  \n",
    "    ax.hlines(0, -10, 10,\n",
    "             color = \"grey\")\n",
    "    \n",
    "    ax.vlines(0, -10, 10,\n",
    "              color = \"grey\")\n",
    "    \n",
    "    if xlim is not None: ax.set_xlim(xlim) # we can set xlim ylim=(-1, 1)\n",
    "    if ylim is not None: ax.set_ylim(ylim)\n",
    "        \n",
    "    ax.plot((-10, 10),\n",
    "            (-10, 10),\n",
    "            color = \"green\") # abline benchmark\n",
    "    \n",
    "    ax.set_xlabel(x_stock_name)\n",
    "    ax.set_ylabel(y_stock_name)\n",
    "    \n",
    "# for box plot\n",
    "\n",
    "(GS_DPC\n",
    "    .plot(kind = \"box\",\n",
    "          figsize = [6,6]\n",
    "         )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9bd8e1",
   "metadata": {},
   "source": [
    "### SMA MOVING AVERAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4aefbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_closing_prices_WIDE[\"TICKER\"].rolling(5).mean().plot(figsize = [16, 8])\n",
    "min_period = 80 #no of days\n",
    "vol = (dpc_method_pct_change.rolling(min_period).std() * np.sqrt(min_period))\n",
    "#rolling stdev\n",
    "rolling_correlations =\\\n",
    "(\n",
    "    daily_closing_prices_WIDE[\"BA\"]\n",
    "    .rolling(22)\n",
    "    .corr(daily_closing_prices_WIDE[\"UAL\"]\n",
    "         )\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    " rolling_correlations # runs from -1 through 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6fb703",
   "metadata": {},
   "source": [
    "### Time series Momentum strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fac1a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The most simple time series momentum strategy is to buy the stock if the last return was positive \n",
    "#and to sell it if it was negative. \n",
    "#With NumPy and pandas, this is easy to formalize; \n",
    "#just take the sign of the last available return as the market position.\n",
    "\n",
    "# to extract data\n",
    "def obtain(tickers,\n",
    "           start,\n",
    "           end):\n",
    "    def data(ticker):\n",
    "        return(pdr\n",
    "               .get_data_yahoo(ticker,\n",
    "                               start = start,\n",
    "                               end = end)\n",
    "              )\n",
    "    interim_data = map(data, tickers)\n",
    "    return(pd\n",
    "          .concat(interim_data,\n",
    "                  keys = tickers,\n",
    "                  names = [\"Tickers\", \"Date\"]\n",
    "                 )\n",
    "          )\n",
    "# collect our data\n",
    "data =\\\n",
    "(\n",
    "    obtain(tickers,\n",
    "            dt.datetime(2013, 9, 9),\n",
    "            dt.datetime(2023, 9, 8)\n",
    "           )\n",
    ")\n",
    "# pivot data\n",
    "data_WIDE =\\\n",
    "(\n",
    "    data\n",
    "    [\"Close\"]\n",
    "    .reset_index()\n",
    "    .pivot(index = \"Date\",\n",
    "           columns = \"Tickers\",\n",
    "           values = \"Close\")\n",
    ")\n",
    "# rename column for convenience\n",
    "qqq =\\\n",
    "(\n",
    "    pd\n",
    "    .DataFrame(data_WIDE[\"QQQ\"]\n",
    "              )\n",
    "    .rename(columns = {\"QQQ\": \"Price\"}\n",
    "           )\n",
    ")\n",
    "# calculate return for passively following the market\n",
    "qqq[\"returns\"] =\\\n",
    "(\n",
    "    np\n",
    "    .log(qqq[\"Price\"]\n",
    "         / \n",
    "         qqq[\"Price\"].shift(1)\n",
    "        )\n",
    ")\n",
    "# determine the sign \n",
    "qqq[\"positions\"] =\\\n",
    "(\n",
    "    np\n",
    "    .sign(qqq[\"returns\"]\n",
    "         )\n",
    ")\n",
    "# multiply our b/s position by the return\n",
    "qqq[\"strategy_returns\"] =\\\n",
    "(\n",
    "    qqq[\"positions\"]\n",
    "    .shift(1)\n",
    "    *\n",
    "    qqq[\"returns\"] # passive following\n",
    ")\n",
    "# plot the graph\n",
    "(\n",
    "    qqq\n",
    "    [[\"returns\", \"strategy_returns\"]] # passive following vs. your strategy (here, time-series momentum)\n",
    "    .dropna()\n",
    "    .cumsum()\n",
    "    .apply(np.exp)\n",
    "    .plot(figsize = [16, 8]\n",
    "         )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2918dec",
   "metadata": {},
   "source": [
    "### Momentum startegy with signals, std threshold and SMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b547c5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MeanReversion(dataframe,SMA,threshold):\n",
    "    \n",
    "    # We would want to remove the unwated columns from original df\n",
    "    if all(col in dataframe.columns for col in ['Open', 'High', 'Low', 'Close', 'Volume']):\n",
    "        dataframe = dataframe.drop(['Open', 'High', 'Low', 'Close', 'Volume'], axis=1)\n",
    "    \n",
    "    #Calculate Simple Moving Average\n",
    "    dataframe[f'SMA_{SMA}'] = \\\n",
    "            dataframe['Adj Close'] \\\n",
    "            .rolling(window=SMA) \\\n",
    "            .mean()\n",
    "\n",
    "    #Create a Threshold \n",
    "    dataframe[f'STD+{threshold}'] = \\\n",
    "        dataframe[f'SMA_{SMA}'] + threshold*(dataframe['Adj Close'].rolling(SMA).std())\n",
    "\n",
    "    dataframe[f'STD-{threshold}'] = \\\n",
    "            dataframe[f'SMA_{SMA}'] - threshold*(dataframe['Adj Close'].rolling(SMA).std())\n",
    "\n",
    "    #Calculate the Distance\n",
    "    dataframe['DISTANCE'] = \\\n",
    "        dataframe['Adj Close'] - dataframe[f'SMA_{SMA}']\n",
    "\n",
    "    #Create the positions\n",
    "    dataframe['POSITION'] = \\\n",
    "        np.where(dataframe['DISTANCE'] > (dataframe[f'STD+{threshold}'] - dataframe[f'SMA_{SMA}']),\n",
    "                 -1, np.nan)\n",
    "\n",
    "    dataframe['POSITION'] = \\\n",
    "        np.where(dataframe['DISTANCE'] < (dataframe[f'STD-{threshold}'] - dataframe[f'SMA_{SMA}']),\n",
    "                 1, dataframe['POSITION'])\n",
    "\n",
    "    dataframe['POSITION'] = \\\n",
    "        np.where(dataframe['DISTANCE'] * dataframe['DISTANCE'].shift(1) < 0,\n",
    "                 0, dataframe['POSITION'])\n",
    "    \n",
    "    #forward fill the NaN Values\n",
    "    dataframe['POSITION'] = \\\n",
    "        dataframe['POSITION'].ffill()\n",
    "    \n",
    "    dataframe['POSITION'] = \\\n",
    "        dataframe['POSITION'].fillna(0)\n",
    "    \n",
    "    #Calculate Returns\n",
    "    dataframe['RETURN'] = \\\n",
    "        np.log(dataframe['Adj Close'] / dataframe['Adj Close'].shift(1))\n",
    "\n",
    "    #Calculate Strategy Returns\n",
    "    dataframe['STRATEGY'] = \\\n",
    "        dataframe['POSITION'].shift(1) * dataframe['RETURN']\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a0bc88",
   "metadata": {},
   "source": [
    "### Plot for the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eef2169",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the figure size\n",
    "fig =\\\n",
    "    (plt\n",
    "     .figure(figsize = [16, 10], \n",
    "             dpi=300\n",
    "            )\n",
    "    )\n",
    "\n",
    "#Create subplot with 1 row and 1 col\n",
    "sub =\\\n",
    "(    fig\n",
    "    .add_subplot(111)\n",
    ")\n",
    "\n",
    "#Plot the GOOGL df\n",
    "(\n",
    "    GOOGL[['Adj Close',\n",
    "       'SMA_42',\n",
    "       'STD+2',\n",
    "       'STD-2']]\n",
    "        .plot(ax = sub,\n",
    "             style = ['-','-','--','--'],\n",
    "             lw = 0.8,\n",
    "             title = 'Google Stock Data',\n",
    "             ylabel = 'Stock Price')\n",
    ")\n",
    "#BUY Signal\n",
    "(\n",
    "    sub\n",
    "    .plot(GOOGL.loc[GOOGL.POSITION == 1].index,\n",
    "         GOOGL[GOOGL.POSITION == 1]['Adj Close'],\n",
    "         'g^',\n",
    "         markersize = 2)\n",
    ")\n",
    "#SELL Signal\n",
    "(\n",
    "    sub\n",
    "    .plot(GOOGL.loc[GOOGL.POSITION == -1].index,\n",
    "         GOOGL[GOOGL.POSITION == -1]['Adj Close'],\n",
    "         'rv',\n",
    "         markersize = 2)\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35abfece",
   "metadata": {},
   "source": [
    "### MOMENTUM WITH DOUBLE SMA AND EMPTY SIGNAL DF WALKTHROUGH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c81b4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "META =(pdr.get_data_yahoo(\"META\", start = datetime.datetime(2014, 3, 1), end = datetime.datetime(2023, 2, 15)))\n",
    "# Setting look back periods\n",
    "short = 20 # make this tunable object (this should be your iterables)\n",
    "long = 60 # make this tunable object\n",
    "BUY_or_SELL =(pd.DataFrame(index = META.index)) #create an empty df\n",
    "BUY_or_SELL[\"BUY_or_SELL\"] = 0.0\n",
    "# Shorter-term SMA\n",
    "BUY_or_SELL[\"shorter_SMA\"] =\\\n",
    "    (\n",
    "    META[\"Close\"]\n",
    "    .rolling(window = short,\n",
    "             min_periods = 1,\n",
    "             center = False)\n",
    "    .mean()\n",
    "    )\n",
    "# Longer-term SMA\n",
    " BUY_or_SELL[\"longer_SMA\"] =\\\n",
    "    (\n",
    "    META[\"Close\"]\n",
    "    .rolling(window = long,\n",
    "             min_periods = 1,\n",
    "             center = False)\n",
    "    .mean()\n",
    "    )\n",
    "#create a signal when SMAs cross\n",
    "BUY_or_SELL[\"BUY_or_SELL\"][short: ] =\\\n",
    "(\n",
    "    np\n",
    "    .where(BUY_or_SELL[\"shorter_SMA\"][short: ] > BUY_or_SELL[\"longer_SMA\"][short: ],\n",
    "           1.0,\n",
    "           0.0\n",
    "           )\n",
    ")\n",
    "# difference the positions\n",
    "BUY_or_SELL[\"Positions\"] =\\\n",
    "(    BUY_or_SELL[\"BUY_or_SELL\"]\n",
    "     .diff()\n",
    ")\n",
    "# plot\n",
    "\n",
    "fig =\\\n",
    "    (plt\n",
    "     .figure(figsize = [16, 10]\n",
    "            )\n",
    "    )\n",
    "\n",
    "sub =\\\n",
    "(    fig\n",
    "    .add_subplot(111,\n",
    "                 ylabel = \"Stock Price\")\n",
    ")\n",
    "\n",
    "(META[\"Close\"]\n",
    " .plot(ax = sub,\n",
    "       color = \"grey\",\n",
    "       lw = 0.80) # This is for closing price\n",
    ")\n",
    "\n",
    "(BUY_or_SELL[[\"shorter_SMA\", \n",
    "              \"longer_SMA\"]]\n",
    "            .plot(ax = sub,\n",
    "                  style = [\"--\", \"--\"],\n",
    "                  lw = 0.80\n",
    "                 )\n",
    ")\n",
    "\n",
    "# Buy\n",
    "\n",
    "(\n",
    "    sub\n",
    "    .plot(BUY_or_SELL.loc[BUY_or_SELL.Positions == 1.0].index,\n",
    "          BUY_or_SELL.shorter_SMA[BUY_or_SELL.Positions == 1.0],\n",
    "          \"^\",\n",
    "          color = \"green\",\n",
    "          markersize = 12)\n",
    "\n",
    ")\n",
    "\n",
    "# # Sell\n",
    "\n",
    "(\n",
    "    sub\n",
    "    .plot(BUY_or_SELL.loc[BUY_or_SELL.Positions == -1.0].index,\n",
    "          BUY_or_SELL.shorter_SMA[BUY_or_SELL.Positions == -1.0],\n",
    "          \"v\",\n",
    "          color = \"red\",\n",
    "          markersize = 12)\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35572f6c",
   "metadata": {},
   "source": [
    "### Mean reversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991a7bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean-reversion strategies anticipate a negative correlation between returns\n",
    "\n",
    "#get data\n",
    "gold =\\\n",
    "(\n",
    "    pdr\n",
    "    .get_data_yahoo(\"GDX\",\n",
    "                    start = dt.datetime(2011, 9, 9),\n",
    "                    end = dt.datetime(2023, 9, 8)\n",
    "                   )\n",
    ")\n",
    "# calculate log returns\n",
    "gold[\"RETURNS\"] =\\\n",
    "(np\n",
    "    .log(gold[\"Close\"] \n",
    "         /\n",
    "         gold[\"Close\"]\n",
    "         .shift(1)\n",
    "        )\n",
    ")\n",
    "# calculate sma, change only the window\n",
    "gold[\"SMA_22\"] =\\\n",
    "(\n",
    "    gold[\"Close\"]\n",
    "    .rolling(window = 22)\n",
    "    .mean()\n",
    ")\n",
    "#set threshold\n",
    "threshold = 3 # 3 dollar deviation\n",
    "#calculate distance\n",
    "gold[\"distance\"] = gold[\"Close\"] - gold[\"SMA_22\"] #how much deviated \n",
    "#SET OUR TRADING POSITIONS\n",
    "gold[\"trading_positions\"] =\\\n",
    "(\n",
    "    np\n",
    "    .where(gold[\"distance\"] > threshold, # overbought --> sell (short) if dev is higher than threshold\n",
    "           -1, np.nan)\n",
    ")\n",
    "\n",
    "gold[\"trading_positions\"] =\\\n",
    "(\n",
    "    np\n",
    "    .where(gold[\"distance\"] < -threshold, # oversold --> buy (long)\n",
    "           1, gold[\"trading_positions\"]\n",
    "          )\n",
    ")\n",
    "\n",
    "gold[\"trading_positions\"] =\\\n",
    "(\n",
    "    np\n",
    "    #           +                            - \n",
    "    #           -                            +\n",
    "    .where(gold[\"distance\"] * gold[\"distance\"].shift(1) < 0, # oversold --> buy (long)\n",
    "           0, gold[\"trading_positions\"]\n",
    "          )\n",
    ")\n",
    "# to replace null values with values of previous day\n",
    "gold[\"trading_positions\"] =\\\n",
    "    (gold[\"trading_positions\"]\n",
    "     .ffill()\n",
    "    )\n",
    "# plot with positions\n",
    "(\n",
    "    gold[\"trading_positions\"]\n",
    "    .iloc[22: ] # NOTE THAT 22 IS WINDOW SIZE AND SUBJECT TO CHANGE\n",
    "    .plot(figsize = [18, 6],\n",
    "          ylim = [-1.10, 1.10]\n",
    "         )\n",
    ")\n",
    "#now time to execute and plot our strategy\n",
    "gold[\"STRATEGY\"] =\\\n",
    "    (\n",
    "    gold\n",
    "    [\"trading_positions\"]\n",
    "    .shift(1)\n",
    "    *\n",
    "    gold[\"RETURNS\"]\n",
    "    )\n",
    "(\n",
    "    gold\n",
    "    [[\"RETURNS\", \"STRATEGY\"]]\n",
    "    .dropna()\n",
    "    .cumsum()\n",
    "    .apply(np.exp)\n",
    "    .plot(figsize = [18, 7]\n",
    "         )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cac088",
   "metadata": {},
   "source": [
    "### Mean reversion with dot b/s signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622f2658",
   "metadata": {},
   "outputs": [],
   "source": [
    "G =\\\n",
    "(\n",
    "    pdr\n",
    "    .get_data_yahoo(\"GDX\",\n",
    "                    dt.datetime(2013, 3, 1),\n",
    "                    dt.datetime(2023, 2, 28)\n",
    "                   )\n",
    ")\n",
    "G[\"returns\"] =\\\n",
    "(\n",
    "    np\n",
    "    .log(G[\"Close\"]\n",
    "         /\n",
    "         G[\"Close\"].shift(1)\n",
    "        )\n",
    ")\n",
    "\n",
    "G[\"SMA_22\"] =\\\n",
    "(\n",
    "    G[\"Close\"]\n",
    "    .rolling(window = 22)\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "G[\"distance\"] = G[\"Close\"] - G[\"SMA_22\"]\n",
    "(\n",
    "G[\"distance\"]\n",
    ".dropna()\n",
    ".plot(figsize = [16, 7],\n",
    "      color = \"grey\",\n",
    "      alpha = 0.80)\n",
    ")\n",
    "\n",
    "# Upper-bound threshold\n",
    "\n",
    "plt.axhline(3,\n",
    "            color = \"blue\",\n",
    "            ls = \"--\")\n",
    "\n",
    "# # Lower-bound threshold\n",
    "\n",
    "plt.axhline(-3,\n",
    "            color = \"blue\",\n",
    "            ls = \"--\")\n",
    "\n",
    "# Sell Signal\n",
    "\n",
    "G[\"positions\"] =\\\n",
    "(\n",
    "    np\n",
    "    .where(G[\"distance\"] > 3,\n",
    "           -1, np.nan)\n",
    ")\n",
    "\n",
    "# Buy Signal\n",
    "\n",
    "G[\"positions\"] =\\\n",
    "(\n",
    "    np\n",
    "    .where(G[\"distance\"] < -3,\n",
    "           1, G[\"positions\"])\n",
    ")\n",
    "\n",
    "# # Market-Neutral Signal\n",
    "\n",
    "G[\"positions\"] =\\\n",
    "(\n",
    "    np\n",
    "    .where(G[\"distance\"] * G[\"distance\"].shift(1) < 0,\n",
    "           0, G[\"positions\"])\n",
    ")\n",
    "\n",
    "(\n",
    "    G[\"positions\"]\n",
    "    .dropna()\n",
    "    .plot(figsize = [16 , 7],\n",
    "          color = \"red\",\n",
    "          style = \"o\",\n",
    "          alpha = 0.30)\n",
    ")\n",
    "\n",
    "#summing up the positions\n",
    "\n",
    "G[\"positions\"] =\\\n",
    "(\n",
    "    np\n",
    "    .where(G[\"distance\"] > 3,\n",
    "           -1, np.nan)\n",
    ")\n",
    "\n",
    "G[\"positions\"] =\\\n",
    "(\n",
    "    np\n",
    "    .where(G[\"distance\"] < -3,\n",
    "           1, G[\"positions\"]\n",
    "          )\n",
    ")\n",
    "\n",
    "G[\"positions\"] =\\\n",
    "(\n",
    "    np\n",
    "    .where(G[\"distance\"] * G[\"distance\"].shift(1) < 0,\n",
    "           0, G[\"positions\"]\n",
    "          )\n",
    ")\n",
    "# don't forget to fillna\n",
    "G[\"positions\"] = (G[\"positions\"].ffill())\n",
    "G[\"positions\"] = (G[\"positions\"].fillna(0))\n",
    "# create strategy\n",
    "G[\"strategy\"] = (G[\"positions\"].shift(1)*G[\"returns\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46928135",
   "metadata": {},
   "source": [
    "#### Max Draw Down (MDD) is defined as largest single drop from peak to bottom in the value of a portfolio before a new peak is achieved. if new peak is achieved, throw away previous peak and make new highest value as peak\n",
    "#### % DROP of Current Value from Max Value = $ \\frac{Current Value}{Max Value} - 1 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8135f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create max value for every window - rolling max\n",
    "Toy_DF_for_MDD[\"rolling_max\"] = (Toy_DF_for_MDD[\"Value\"].rolling(window = 3, min_periods = 1).max())\n",
    "Toy_DF_for_MDD[\"annual_drawdown\"] = (Toy_DF_for_MDD[\"Value\"]/Toy_DF_for_MDD[\"rolling_max\"]- 1.0)\n",
    "Toy_DF_for_MDD[\"annual_max_drawdown\"] = (Toy_DF_for_MDD[\"annual_drawdown\"].rolling(window = 3, min_periods = 1).min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933f6dad",
   "metadata": {},
   "source": [
    "### SMA CROSSING WITH -1, 1 B/S SIGNALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c771c51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # short-term simple moving averages = 40 days\n",
    "# # long-term simple moving averages = 250 days\n",
    "\n",
    "# # SSMA\n",
    "\n",
    "fx[\"SMA_40\"] = (fx[\"Price\"].rolling(window = 40).mean())\n",
    "# # LSMA\n",
    "fx[\"SMA_250\"] = (fx[\"Price\"].rolling(250).mean())\n",
    "(fx.plot(title = \"USD/EUR SMAs (40 vs. 250 days)\", figsize = [18, 6]))\n",
    "#when short term avg is less than long term - sell, vice versa -  buy\n",
    "fx[\"positions\"] = (np.where(fx[\"SMA_40\"] > fx[\"SMA_250\"], 1, -1)) #golden, dead\n",
    "fx = (fx.dropna())\n",
    "#don't forget to dropna\n",
    "ax = (fx[[\"Price\", \"SMA_40\", \"SMA_250\", \"positions\"]].plot(secondary_y = \"positions\",style = [\"grey\", \"green\",\"red\",\"blue\"], figsize = [18, 6]))\n",
    "(ax.legend(loc = \"upper center\", shadow = True, ncol = 4, bbox_to_anchor = (0.55, 1.10), fancybox = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af887582",
   "metadata": {},
   "source": [
    "### Market and strategy log returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c6e3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "fx[\"log_returns\"] = (np.log(fx[\"Price\"] / fx[\"Price\"].shift(1)))\n",
    "# log returns bc statistical property, var to daily return is volatility. return = normal distr -> returns are lognormal\n",
    "# muliply the position with the returns column, shifted by 1; note that the log r are additive\n",
    "(fx[\"log_returns\"].hist(bins = 50))\n",
    "fx[\"strategy_returns\"] =\\\n",
    "(fx[\"positions\"].shift(1) * fx[\"log_returns\"]) #to calculate strategy return\n",
    "(fx[[\"log_returns\", \"strategy_returns\"]].sum()).apply(np.exp) #to report to the shareholders\n",
    "(fx[[\"log_returns\", \"strategy_returns\"]].cumsum().apply(np.exp)).plot(figsize = [18, 8]) #to plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5916e1eb",
   "metadata": {},
   "source": [
    "### Performance metrics based on log returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0ee7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's calculate performance metrics\n",
    "(fx[[\"log_returns\", \"strategy_returns\"]].mean()*252)\n",
    "# Bring it back to original scale\n",
    "np.exp(fx[[\"log_returns\", \"strategy_returns\"]].mean()*252) - 1\n",
    "(fx[[\"log_returns\", \"strategy_returns\"]].std()* 252** 0.5)\n",
    "fx[\"cumulative_returns\"] = (fx[\"strategy_returns\"].cumsum().apply(np.exp))\n",
    "fx[\"max_gross_performance\"] = (fx[\"cumulative_returns\"].cummax())\n",
    "#let's plot for MDD\n",
    "drawdown = fx[\"max_gross_performance\"] - fx[\"cumulative_returns\"]\n",
    "drawdown.max() #mdd in %\n",
    "periods =\\\n",
    "(drawdown[drawdown == 0].index[ 1 :   ].to_pydatetime() - drawdown[drawdown == 0].index[   : -1].to_pydatetime())\n",
    "periods.max() #mdd in days\n",
    "(fx[[\"cumulative_returns\", \"max_gross_performance\"]].dropna().plot(figsize = [18, 8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c67098",
   "metadata": {},
   "source": [
    "### Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5100cc1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m Capital \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5e6\u001b[39m \u001b[38;5;66;03m#our cap\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m Position \u001b[38;5;241m=\u001b[39m (pd\u001b[38;5;241m.\u001b[39mDataFrame(index \u001b[38;5;241m=\u001b[39m BUY_or_SELL\u001b[38;5;241m.\u001b[39mindex)\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0.0\u001b[39m)) \u001b[38;5;66;03m#empty df for positions\u001b[39;00m\n\u001b[0;32m      3\u001b[0m Position[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMETA\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m*\u001b[39m BUY_or_SELL[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBUY_or_SELL\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;66;03m#where 200 is our no of shares\u001b[39;00m\n\u001b[0;32m      4\u001b[0m Portfolio \u001b[38;5;241m=\u001b[39m (Position\u001b[38;5;241m.\u001b[39mmultiply(META[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdj Close\u001b[39m\u001b[38;5;124m\"\u001b[39m], axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)) \u001b[38;5;66;03m#empty df to store market value of open positions\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "Capital = 5e6 #our cap\n",
    "Position = (pd.DataFrame(index = BUY_or_SELL.index).fillna(0.0)) #empty df for positions\n",
    "Position[\"META\"] = (200 * BUY_or_SELL[\"BUY_or_SELL\"]) #where 200 is our no of shares\n",
    "Portfolio = (Position.multiply(META[\"Adj Close\"], axis = 0)) #empty df to store market value of open positions\n",
    "difference_in_shares_owned = (Position.diff()) #empty df that stores the diff in shares owned\n",
    "Portfolio[\"our_holdings\"] = (Position.multiply(META[\"Adj Close\"], axis = 0)).sum(axis = 1)\n",
    "#this column multiplies the stocks we hold by their adj close\n",
    "Portfolio[\"our_cash\"] = (Capital - (difference_in_shares_owned.multiply(META[\"Adj Close\"], axis = 0)).sum(axis = 1).cumsum())\n",
    "#this is our initial cap minus price paid for stock, the funds we have at this point\n",
    "Portfolio[\"total\"] = Portfolio[\"our_cash\"] + Portfolio[\"our_holdings\"] #cash+shares\n",
    "Portfolio[\"returns\"] = (Portfolio[\"total\"].pct_change()) #%change of total value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae0e627",
   "metadata": {},
   "source": [
    "### Calculation of Cash and Strategy return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bcfe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalculateCashReturn(dataframe,capital):\n",
    "    \n",
    "    #Calculate cumulative return --> only for our comparison with strategy returns\n",
    "    dataframe['CUMULATIVE RETURN'] = \\\n",
    "        dataframe['RETURN'].cumsum().apply(np.exp)\n",
    "    \n",
    "    dataframe['CUMULATIVE STRATEGY'] = \\\n",
    "        dataframe['STRATEGY'].cumsum().apply(np.exp)\n",
    "    \n",
    "    #Final cash return\n",
    "    capital = \\\n",
    "        capital * dataframe['CUMULATIVE STRATEGY'][-1]\n",
    "\n",
    "    return capital"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc05f4b",
   "metadata": {},
   "source": [
    "### Plotting the backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d15ed3b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m fig \u001b[38;5;241m=\u001b[39m\\\n\u001b[1;32m----> 2\u001b[0m     (plt\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m      3\u001b[0m      figure(figsize \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m10\u001b[39m]\n\u001b[0;32m      4\u001b[0m            )\n\u001b[0;32m      5\u001b[0m     )\n\u001b[0;32m      7\u001b[0m sub \u001b[38;5;241m=\u001b[39m\\\n\u001b[0;32m      8\u001b[0m     (fig\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;241m.\u001b[39madd_subplot(\u001b[38;5;241m111\u001b[39m,\n\u001b[0;32m     10\u001b[0m                  ylabel \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValue of Our Portfolio (USD)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m     )\n\u001b[0;32m     13\u001b[0m (\n\u001b[0;32m     14\u001b[0m     Portfolio[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;241m.\u001b[39mplot(ax \u001b[38;5;241m=\u001b[39m sub,\n\u001b[0;32m     16\u001b[0m           color \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrey\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m           lw \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.80\u001b[39m)\n\u001b[0;32m     18\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig = (plt.figure(figsize = [16, 10]))\n",
    "sub = (fig.add_subplot(111, ylabel = \"Value of Our Portfolio (USD)\"))\n",
    "(Portfolio[\"total\"].plot(ax = sub, color = \"grey\", lw = 0.80))\n",
    "# Buy\n",
    "(sub.plot(Portfolio.loc[BUY_or_SELL.Positions == 1.0].index, Portfolio.total[BUY_or_SELL.Positions == 1.0],\"^\", color = \"green\", markersize = 10))\n",
    "# Sell\n",
    "(sub.plot(Portfolio.loc[BUY_or_SELL.Positions == -1.0].index, Portfolio.total[BUY_or_SELL.Positions == -1.0], \"v\", color = \"red\", markersize = 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63f6d1c",
   "metadata": {},
   "source": [
    "### Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c75c206",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sharpe = (np.sqrt(253) * (Portfolio[\"returns\"].mean() / Portfolio[\"returns\"].std())) #returns are % returns, see above\n",
    "#note that this Sharpe ratio is annualized - 253 is the no of trading days!\n",
    "\n",
    "#MDD - MAXIMUM DRAWDOWN - indicates our risk, how long an aset has been falling from a peak before it got to another peak\n",
    "#basically our largest single drop from peak to bottom\n",
    "\n",
    "# first calculate rolling max\n",
    "window = 253\n",
    "rolling_max = (apple[\"Close\"].rolling(window = window, min_periods = 1).max())\n",
    "daily_drawdown = (apple[\"Close\"] / rolling_max) #when negative, means it is below rolling max\n",
    "max_daily_drawdown = (daily_drawdown.rolling(window = window, min_periods = 1).max()) #rolling mdd for previous window days\n",
    "#let's plot\n",
    "fig = plt.figure(figsize = [16, 8])\n",
    "# rolling_max.plot(color=\"green\", lw = 0.80)\n",
    "daily_drawdown.plot(color = \"grey\", lw = 0.80)\n",
    "max_daily_drawdown.plot(color = \"red\",lw = 0.80)\n",
    "plt.show()\n",
    "\n",
    "#CAGR - compound annual growth rate, rate of return over time\n",
    "days = ((META.index[-1] - META.index[0]).days)\n",
    "CAGR = ((((META[\"Adj Close\"][-1]) / (META[\"Adj Close\"][0]))**(365.0/days)) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944529a3",
   "metadata": {},
   "source": [
    "### OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c2fbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation\n",
    "\n",
    "# Define X variable\n",
    "Explnatory_Variable = (sm.add_constant(dpc_method_pct_change[\"ticker\"]) #here the df is just % change of adj close\n",
    "Model = (sm.OLS(dpc_method_pct_change[\"ticker\"], Explnatory_Variable).fit())\n",
    "dir(Model)\n",
    "Model.summary()\n",
    "for attributes in dir(Model):\n",
    "     if not attributes.startswith(\"_\"):\n",
    "         print(attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667c84ea",
   "metadata": {},
   "source": [
    "### OLS revised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0725d1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create x\n",
    "x = (np.linspace(0, 10))\n",
    "import random\n",
    "def setting_seed(seed = 100):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "setting_seed(230919)\n",
    "# create y\n",
    "y = x + np.random.standard_normal(len(x))\n",
    "ols = np.polyfit(x, y, deg = 1) #we regress y on x\n",
    "# parameter coefficients learn directly from data\n",
    "# hyperparemater we use data to find them \n",
    "ols \n",
    "#first element here is slope, second - the intercept\n",
    "plt.figure(figsize = (16,10))\n",
    "plt.plot(x, y, \"ro\", label = \"Our Data\")\n",
    "plt.plot(x, np.polyval(ols, x), \"b\", label = \"Linear Regression\") # estimation\n",
    "plt.legend(loc = 0)\n",
    "# to extrapolate ols, we need to enlarge initial interval set with linspace\n",
    "plt.figure(figsize = (16,10))\n",
    "plt.plot(x, y, \"ro\", label = \"Our Data\")\n",
    "x_extended = np.linspace(0, 20)\n",
    "plt.plot(x_extended, np.polyval(ols, x_extended), \"b\", label = \"Extrapolated Linear Regression\")\n",
    "plt.legend(loc = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3ab3e7",
   "metadata": {},
   "source": [
    "### ARIMA - A TIME SERIES MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743383b5",
   "metadata": {},
   "source": [
    "> p denotes the order of Auto Regression (AR) polynomials\n",
    "\n",
    "> d denotes the number of nonseasonal differences needed for stationarity\n",
    "\n",
    "> q denotes the oder of Moving Average (MA) polynomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71c56e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Basic Set-up for ARIMA - don't forget to split X dataset!\n",
    "\n",
    "X_train_ARIMA = (X_train.loc[ : , [\"GOOGL\", \"IBM\", \"DEXJPUS\", \"SP500\", \"DJIA\", \"VIXCLS\"]]) \n",
    "X_test_ARIMA = (X_test.loc[ : , [\"GOOGL\", \"IBM\", \"DEXJPUS\", \"SP500\", \"DJIA\", \"VIXCLS\"]])    \n",
    "train_len = len(X_train_ARIMA)\n",
    "test_len = len(X_test_ARIMA)\n",
    "total_len = len(X)\n",
    "\n",
    "# set p, d, q - they are in order\n",
    "modelARIMA = (stats.ARIMA(endog = Y_train, exog = X_train_ARIMA, order = [1, 0, 0]))\n",
    "model_fit = modelARIMA.fit()\n",
    "error_training_ARIMA = (mean_squared_error(Y_train, model_fit.fittedvalues))\n",
    "predicted = (model_fit.predict(start = train_len - 1, end = total_len - 1, exog = X_test_ARIMA)[1: ])\n",
    "error_testing_ARIMA = (mean_squared_error(Y_test, predicted))\n",
    "\n",
    "# append error testing arima to previous results\n",
    "test_results.append(error_testing_ARIMA)\n",
    "train_results.append(error_training_ARIMA)\n",
    "names.append(\"ARIMA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19c439f",
   "metadata": {},
   "source": [
    "### Hyperparameter testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7db1894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning; Grid Search for ARIMA\n",
    "\n",
    "def assess_ARIMA_model(arima_order):\n",
    "    modelARIMA = stats.ARIMA(endog = Y_train, exog = X_train_ARIMA, order = arima_order)\n",
    "    model_fit = modelARIMA.fit()\n",
    "    error = mean_squared_error(Y_train, model_fit.fittedvalues)\n",
    "    return error\n",
    "def assess_models(p_values, d_values, q_values):\n",
    "    best_score, best_cfg = float(\"inf\"), None\n",
    "    for p in p_values:\n",
    "        for d in d_values:\n",
    "            for q in q_values:\n",
    "                order = (p, d, q)\n",
    "                try:\n",
    "                    mse = assess_ARIMA_model(order)\n",
    "                    if mse < best_score:\n",
    "                        best_score, best_cfg = mse, order\n",
    "                    print(\"ARIMA%s MSE = %.7f\" % (order, mse))\n",
    "                except:\n",
    "                    continue\n",
    "    print(\"Best ARIMA%s MSE = %.7f\" % (best_cfg, best_score))\n",
    "    \n",
    "# parameters to use for assessment\n",
    "p_values = [0, 1, 2]\n",
    "d_values = range(0, 2)\n",
    "q_values = range(0, 2)\n",
    "assess_models(p_values, d_values, q_values)\n",
    "\n",
    "ARIMA_Tuned = stats.ARIMA(endog = Y_train, exog = X_train_ARIMA, order = [])\n",
    "                          # input optimal set of hyperparameters here^\n",
    "ARIMA_Fit_Tuned = ARIMA_Tuned.fit()\n",
    "# let's calculate accuracy\n",
    "Predicted_Tuned = model_fit.predict(start = train_len - 1, end = total_len - 1, exog = X_test_ARIMA)[1:]\n",
    "# `model_fit` is a fitted ARIMA model instance.\n",
    "# `model_fit.predict()` is used to make predictions based on the fitted model.\n",
    "#`start=train_len - 1` specifies the starting point of the prediction. \n",
    "# Our code is setting it to one less than the length of the training data.\n",
    "# `end=total_len - 1` sets the endpoint of the prediction. It is one less than the total length of the dataset, \n",
    "#indicating that predictions will be made for the entire testing set.\n",
    "# `exog=X_test_ARIMA` refers to exogenous variables, which are external factors that can influence the predictions. \n",
    "# X_test_ARIMA is presumably the testing set of these exogenous variables.\n",
    "# `[1:]` is used to exclude the first element from the predicted values. \n",
    "# It might be done to align the predicted values with the actual values, especially if the prediction \n",
    "# includes the last observation from the training set.\n",
    "\n",
    "# print out the error\n",
    "print(mean_squared_error(Y_test,Predicted_Tuned))\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize = (16, 10))\n",
    "Predicted_Tuned.index = Y_test.index\n",
    "plt.plot(np.exp(Y_test).cumprod(), \"b--\", label = \"Actual Y\")\n",
    "plt.plot(np.exp(Predicted_Tuned).cumprod(), \"r\", label = \"Predicted Y (Y hat)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8d57da",
   "metadata": {},
   "source": [
    "### Lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aa9367",
   "metadata": {},
   "outputs": [],
   "source": [
    "GE = (pdr.get_data_yahoo('GE', start, end))\n",
    "# calculate log return\n",
    "GE['RETURN'] = np.log(GE['Adj Close']/GE['Adj Close'].shift(1))\n",
    "# Lets define a function to create lags. We pass in two values, our dataframe and number of\n",
    "# lags we want\n",
    "def CreateLags(dataframe, LAGS):\n",
    "    #Empty List to store column names\n",
    "    COLS = []\n",
    "    #Empty Dataframe\n",
    "    lag_df = pd.DataFrame(index = dataframe.index)\n",
    "    #Fetch Returns from original df\n",
    "    lag_df['RETURN'] = dataframe['RETURN']\n",
    "    #Create lags\n",
    "    for i in range(1,LAGS+1):\n",
    "        COL = f'Lag_{i}'\n",
    "        lag_df[COL] = lag_df['RETURN'].shift(i)\n",
    "        COLS.append(COL)\n",
    "    return lag_df.dropna(), COLS\n",
    "# Lets define a OLSRegression function to run a regression on a data and append the results to the\n",
    "# origianl dataframe\n",
    "def OLSRegression(dataframe,columns):\n",
    "    # Regress \n",
    "    OLS = np.linalg.lstsq(dataframe[columns], dataframe[\"RETURN\"],rcond = None)[0]\n",
    "    # Calculate the prediction by taking dot product with coefficients\n",
    "    dataframe[\"PREDICTION\"] = np.dot(dataframe[columns], OLS)\n",
    "    return dataframe\n",
    "# this is e.g. for 3 lags\n",
    "GE_LAG_3, COLS_3 = CreateLags(GE, 3)\n",
    "GE_LAG_3 = OLSRegression(GE_LAG_3,COLS_3)\n",
    "# to calculate accuracy of our prediction - we need a sign\n",
    "# if sign matches then we get value 1 when mismatch -1\n",
    "ACCURACY = np.sign(USD_EUR[\"RETURN\"] * USD_EUR[\"PREDICTION\"] ).value_counts()\n",
    "ACCURACY.values[0] / sum(ACCURACY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f0b734",
   "metadata": {},
   "source": [
    "### Vectorized backtesting of a regression-based strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0135a7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "USD_EUR[\"STRATEGY\"] = USD_EUR[\"PREDICTION\"] * USD_EUR[\"RETURN\"]\n",
    "USD_EUR[[\"RETURN\", \"STRATEGY\"]].sum().apply(np.exp)\n",
    "USD_EUR[[\"RETURN\", \"STRATEGY\"]].dropna().cumsum().apply(np.exp).plot(figsize = (16,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf116f1",
   "metadata": {},
   "source": [
    "### Market prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccab1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGRESSION = np.linalg.lstsq(USD_EUR[COLS], np.sign(USD_EUR[\"RETURN\"]), rcond = None)[0]\n",
    "USD_EUR[\"PREDICTION\"] = np.sign(np.dot(USD_EUR[COLS], REGRESSION))\n",
    "HIT_RATIO = np.sign(USD_EUR[\"RETURN\"] * USD_EUR[\"PREDICTION\"]).value_counts()\n",
    "HIT_RATIO.values[0] / sum(HIT_RATIO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b54d24",
   "metadata": {},
   "source": [
    "### Market prediction using ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f54b6b",
   "metadata": {},
   "source": [
    "> Step 1: Model selection | A model is to be picked and instantiated.\n",
    "\n",
    ">Step 2: Model fitting | The model is to be fitted to the data at hand.\n",
    "\n",
    ">Step 3: Prediction | Given the fitted model, the prediction is conducted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cca61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.arange(12)\n",
    "lags = 3\n",
    "matrix = np.zeros((lags + 1, len(data) - lags))\n",
    "LM = linear_model.LinearRegression()\n",
    "LM.fit(matrix[:lags].T, matrix[lags])\n",
    "LM.coef_\n",
    "LM.intercept_\n",
    "LM.predict(matrix[:lags].T)\n",
    "LM = linear_model.LinearRegression(fit_intercept = False)\n",
    "LM.fit(matrix[:lags].T, matrix[lags])\n",
    "LM.coef_\n",
    "LM.intercept_\n",
    "LM.predict(matrix[:lags].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94effaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOLD = pdr.get_data_yahoo(\"GLD\", \n",
    "                          start = dt.datetime(2010, 10, 1),\n",
    "                          end = dt.datetime(2022, 10, 19)\n",
    "                         )\n",
    "GOLD[\"RETURN\"] = np.log(GOLD[\"Close\"] / GOLD[\"Close\"].shift(1))\n",
    "GOLD.dropna(inplace = True)\n",
    "lags = 3\n",
    "cols = []\n",
    "for lag in range(1, lags +1):\n",
    "    col = \"lag_{}\".format(lag)\n",
    "    GOLD[col] = GOLD[\"RETURN\"].shift(lag)\n",
    "    cols.append(col)\n",
    "GOLD.dropna(inplace = True)\n",
    "from sklearn.metrics import accuracy_score\n",
    "M = linear_model.LogisticRegression(solver = \"lbfgs\", C = 1e7, multi_class = \"auto\", max_iter = 1000)\n",
    "M.fit(GOLD[cols],np.sign(GOLD[\"RETURN\"]))\n",
    "GOLD[\"PREDICTION\"] = M.predict(GOLD[cols])\n",
    "GOLD[\"PREDICTION\"].value_counts()\n",
    "accuracy = np.sign(GOLD[\"RETURN\"].iloc[lags:] * GOLD[\"PREDICTION\"].iloc[lags:]).value_counts()\n",
    "accuracy_score(GOLD[\"PREDICTION\"], np.sign(GOLD[\"RETURN\"]))\n",
    "GOLD[\"STRATEGY\"] = GOLD[\"PREDICTION\"] * GOLD[\"RETURN\"]\n",
    "GOLD[[\"RETURN\", \"STRATEGY\"]].sum().apply(np.exp)\n",
    "GOLD[[\"RETURN\", \"STRATEGY\"]].cumsum().apply(np.exp).plot(figsize = (16, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656e4e60",
   "metadata": {},
   "source": [
    "### Unsupervised Machine Learning on example of Dow Jones industrial average index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1748bb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = (dow.isnull() .mean().sort_values(ascending = False)) # True (1) vs. False (0)\n",
    "# our features must be standartised and have not so many missing values\n",
    "drop_list = (sorted(list(missing_values[missing_values > 0.30].index)))\n",
    "dow = (dow.drop(labels = drop_list, axis = 1))\n",
    "# then fill the rest of nulls with available data\n",
    "dow = (dow.fillna(method = \"ffill\"))\n",
    "dow = (dow.dropna(axis = 0))\n",
    "Daily_Linear_Return = (dow.pct_change(1)) # for eigenportfolio normal returns make more sense than logreturns\n",
    "# Operational defition of outliers = data points beyond 3 SD\n",
    "Daily_Linear_Return = (Daily_Linear_Return[Daily_Linear_Return.apply(lambda x:(x - x.mean()).abs() < (3 * x.std())).all(1)])\n",
    "# !!!!! BEFOR PCA ALL VARS SHOULD BE ON THE SAME SCALE so standartize pls via scaler\n",
    "# else some will have more effect than the other....\n",
    "# standartisation = normal distribution with mean 0 and stdev 1\n",
    "scaler = (StandardScaler().fit(Daily_Linear_Return)) # this is our plan tho\n",
    "scaled_dow = (pd.DataFrame(scaler.fit_transform(Daily_Linear_Return), #  and this is execution\n",
    "               columns = Daily_Linear_Return.columns,\n",
    "               index = Daily_Linear_Return.index))\n",
    "scaled_dow.dtypes # check data types - all need to be float before PCA\n",
    "# let's plot linear return\n",
    "plt.figure(figsize = [16, 6])\n",
    "plt.title(\"AAPL Return\")\n",
    "plt.ylabel(\"Linear Return\")\n",
    "(scaled_dow[\"AAPL\"].plot())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65a6611",
   "metadata": {},
   "source": [
    "### Modelling of perfect portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33afdcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data split\n",
    "prop = int(len(scaled_dow) * 0.80)\n",
    "X_Train = scaled_dow[    : prop] # First 80% of the data\n",
    "X_Test  = scaled_dow[prop:     ] # Remaining 20% of the data\n",
    "X_Train_Raw = Daily_Linear_Return[    :prop]\n",
    "X_Test_Raw  = Daily_Linear_Return[prop:    ]\n",
    "stock_tickers = (scaled_dow.columns.values)\n",
    "pca = PCA()\n",
    "PrincipalComponent = pca.fit(X_Train)\n",
    "pca.components_[0] # pca.components is a matrix where each row is pc. [0] is first row and the first pc that we need\n",
    "# let's calculate the explained variance (so, our correlation to the principal component)\n",
    "# eigenvectors with lowest values can be dropped \n",
    "NumEigenValues = 10 # replace 10 with your number\n",
    "fig, axes = (plt.subplots(ncols = 2,figsize = [16, 6]))\n",
    "# Plot on the left panel\n",
    "Series1 =\\\n",
    "(    pd\n",
    "    .Series(pca\n",
    "            .explained_variance_ratio_[ :NumEigenValues])\n",
    "    .sort_values()* 100)\n",
    "# Plot on the right panel\n",
    "Series2 =\\\n",
    "(    pd\n",
    "    .Series(pca\n",
    "            .explained_variance_ratio_[ :NumEigenValues]\n",
    "           ).cumsum()* 100)\n",
    "(Series1\n",
    "    .plot\n",
    "    .barh(ylim = (0, 9),\n",
    "          title = \"Explained Variance Ratio by Top 10 PCs\",\n",
    "          ax = axes[0]))\n",
    "(Series2\n",
    "    .plot(ylim = (0, 100),\n",
    "          xlim = (0, 9),\n",
    "          title = \"Cumulative Explained Variance by Each PC\",\n",
    "          ax = axes[1]))\n",
    "# same but in form of gf with %\n",
    "(pd.Series(np.cumsum(pca.explained_variance_ratio_)).to_frame(\"Explained Variance\").head(NumEigenValues).style.format(\"{:,.2%}\".format))\n",
    "# let's write a function to calculate weights\n",
    "def PCWeights():\n",
    "    weights = pd.DataFrame()\n",
    "    for i in range(len(pca.components_)):\n",
    "        weights[\"weights_{}\".format(i)] = pca.components_[i] / sum(pca.components_[i])\n",
    "    weights = weights.values.T\n",
    "    return weights \n",
    "weights = PCWeights()\n",
    "\n",
    "# let's construct 5 portfolios with weights\n",
    "# scree plot method\n",
    "# Set the number of principal components to be considered \n",
    "NumComponents = 5 # replace 5 with any number\n",
    "# Extract the top principal components from the PCA object\n",
    "# and create a DataFrame with columns named after the original features\n",
    "topPortfolios = (pd.DataFrame(pca.components_[ : NumComponents], columns = dow.columns))\n",
    "# Normalize the weights of the top portfolios such that the weights sum up to 1 for each portfolio\n",
    "# This is done by dividing each weight by the sum of weights for the respective portfolio\n",
    "eigen_portfolios = (topPortfolios.div(topPortfolios.sum(1), axis = 0))\n",
    "# Rename the index of the eigen_portfolios DataFrame for better readability\n",
    "eigen_portfolios.index = [f\"Portfolio {i}\" for i in range(NumComponents)]\n",
    "# Calculate the square root of the explained variance for each component\n",
    "# This provides the standard deviation of returns for each eigenportfolio\n",
    "np.sqrt(pca.explained_variance_)\n",
    "\n",
    "# let's plot! it shows the contribution of each portf to eigenvector\n",
    "(eigen_portfolios\n",
    "    .T  # Transpose the DataFrame to have portfolios as columns and assets as rows\n",
    "    .plot\n",
    "    .bar(subplots = True,\n",
    "         layout = (int(NumComponents), 1),\n",
    "         legend = False,\n",
    "         sharey = True,\n",
    "         figsize = [16, 20], ylim = [-1, 1]))\n",
    "# heatmap\n",
    "plt.figure(figsize = [16, 6])\n",
    "sns.heatmap(topPortfolios, cmap = \"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf95080",
   "metadata": {},
   "source": [
    "### Sharpe for Eigen portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c342de5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sharpe_ratio(ts_returns, periods_per_year = 252):\n",
    "    n_years = ts_returns.shape[0] / periods_per_year\n",
    "    annualized_return = np.power(np.prod(1 + ts_returns), (1 / n_years)\n",
    "                                ) - 1\n",
    "    annualized_vol = ts_returns.std() * np.sqrt(periods_per_year)\n",
    "    annualized_sharpe = annualized_return / annualized_vol\n",
    "    return annualized_return, annualized_vol, annualized_sharpe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6722cf4",
   "metadata": {},
   "source": [
    "### EXAMINABLE EIGEN STUFF PROF'S GIFT TO US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e02df3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gift\n",
    "\n",
    "def recommend_optimal_portfolio():\n",
    "#     Number of eigenportfolios or principal components\n",
    "    n_portfolios = len(pca.components_)\n",
    "    # Initialize arrays for annualized return, volatility, and Sharpe ratio of each eigenportfolio\n",
    "    annualized_ret = np.array([0.] * n_portfolios)\n",
    "    sharpe_metric = np.array([0.] * n_portfolios)\n",
    "    annualized_vol = np.array([0.] * n_portfolios)\n",
    "    # Variable to track the index of the eigenportfolio with the highest Sharpe ratio\n",
    "    highest_sharpe = 0\n",
    "    # Extract stock tickers from the scaled data\n",
    "    stock_tickers =\\\n",
    "    (scaled_dow\n",
    "     .columns \n",
    "     .values)\n",
    "    n_tickers = len(stock_tickers)\n",
    "    # Extract principal components\n",
    "    PCs = pca.components_\n",
    "    # Loop through each eigenportfolio\n",
    "    for i in range(n_portfolios):\n",
    "        # Normalize the weights of the i-th eigenportfolio\n",
    "        pc_w = PCs[i] / sum(PCs[i])\n",
    "        # Create a DataFrame for the eigenportfolio weights\n",
    "        eigen_prtfi =\\\n",
    "            (\n",
    "                pd\n",
    "                .DataFrame(data = {\"weights\": pc_w.squeeze() * 100},\n",
    "                           index = stock_tickers)\n",
    "            )\n",
    "        # Calculate returns for the eigenportfolio\n",
    "        eigen_prtfi.sort_values(by = [\"weights\"],\n",
    "                                ascending = False,\n",
    "                                inplace = True)\n",
    "        eigen_prti_returns =\\\n",
    "            (\n",
    "                np\n",
    "                .dot(X_Train_Raw.loc[ : , eigen_prtfi.index],\n",
    "                     pc_w)\n",
    "            )\n",
    "        eigen_prti_returns =\\\n",
    "            (\n",
    "                pd\n",
    "                .Series(eigen_prti_returns.squeeze(),\n",
    "                        index = X_Train_Raw.index)\n",
    "            )\n",
    "        # Calculate annualized return, volatility, and Sharpe ratio for the eigenportfolio\n",
    "        er, vol, sharpe = calculate_sharpe_ratio(eigen_prti_returns)\n",
    "        # Store the metrics in their respective arrays\n",
    "        annualized_ret[i] = er\n",
    "        annualized_vol[i] = vol\n",
    "        sharpe_metric[i] = sharpe\n",
    "        # Replace NaN values in Sharpe metric array with zeros\n",
    "        sharpe_metric = np.nan_to_num(sharpe_metric)\n",
    "    # Let's find a portfolio with the HIGHEST Sharpe Ratio\n",
    "    highest_sharpe = np.argmax(sharpe_metric)\n",
    "    # Print the details of the eigenportfolio with the highest Sharpe ratio\n",
    "    print(\"Our Eigen Portfolio #%d with the highest Sharpe\\\n",
    "           \\nReturn %.2f%%,\\vol = %.2f%%, \\nSharpe = %.2f\" %\n",
    "         (highest_sharpe,\n",
    "          annualized_ret[highest_sharpe] * 100,\n",
    "          annualized_vol[highest_sharpe] * 100,\n",
    "          sharpe_metric[highest_sharpe]\n",
    "         )\n",
    "         )\n",
    "    # Create a DataFrame to store the results for all eigenportfolios\n",
    "    results =\\\n",
    "        (\n",
    "            pd\n",
    "            .DataFrame(data = {\"Return\": annualized_ret,\n",
    "                               \"Vol\": annualized_vol,\n",
    "                               \"Sharpe\": sharpe_metric}\n",
    "                      )\n",
    "        )\n",
    "    results.dropna(inplace = True)\n",
    "    results.sort_values(by = [\"Sharpe\"],\n",
    "                        ascending = False,\n",
    "                        inplace = True)\n",
    "    # Print the top 10 eigenportfolios based on Sharpe ratio\n",
    "    print(results.head(10)\n",
    "         )\n",
    "    \n",
    "    # FOR PLOTTING\n",
    "    \n",
    "def FindPortfolioVisual():\n",
    "    \n",
    "    n_portfolios = len(pca.components_)\n",
    "    \n",
    "    annualized_ret = np.array([0.] * n_portfolios)\n",
    "    \n",
    "    sharpe_metric = np.array([0.] * n_portfolios)\n",
    "    \n",
    "    annualized_vol = np.array([0.] * n_portfolios)\n",
    "    \n",
    "    highest_sharpe = 0\n",
    "    \n",
    "    stock_tickers = scaled_dow.columns.values\n",
    "    \n",
    "    n_tickers = len(stock_tickers)\n",
    "    \n",
    "    PCs = pca.components_\n",
    "    \n",
    "    for i in range(n_portfolios):\n",
    "        \n",
    "        pc_w = PCs[i] / sum(PCs[i]\n",
    "                           )\n",
    "        \n",
    "        eigen_prtfi = pd.DataFrame(data = {\"weights\": pc_w.squeeze()*100}, \n",
    "                                   index = stock_tickers)\n",
    "        \n",
    "        eigen_prtfi.sort_values(by = [\"weights\"],\n",
    "                                ascending = False,\n",
    "                                inplace = True)\n",
    "        \n",
    "        eigen_prti_returns = np.dot(X_Train_Raw.loc[:, eigen_prtfi.index], \n",
    "                                    pc_w)\n",
    "        \n",
    "        eigen_prti_returns = pd.Series(eigen_prti_returns.squeeze(),\n",
    "                                       index = X_Train_Raw.index)\n",
    "        \n",
    "        er, vol, sharpe = calculate_sharpe_ratio(eigen_prti_returns)\n",
    "        \n",
    "        annualized_ret[i] = er\n",
    "        \n",
    "        annualized_vol[i] = vol\n",
    "       \n",
    "        sharpe_metric[i] = sharpe\n",
    "        \n",
    "        sharpe_metric = np.nan_to_num(sharpe_metric)\n",
    "        \n",
    "    # HOW TO FIND A PORTFOLIO with the HIGHEST Sharpe Ratio\n",
    "    \n",
    "    highest_sharpe = np.argmax(sharpe_metric)\n",
    "    \n",
    "    print(\"Our Eigen Portfolio #%d with the highest Sharpe. Return %.2f%%, vol = %.2f%%, Sharpe = %.2f\" %\n",
    "           (highest_sharpe,\n",
    "           annualized_ret[highest_sharpe]*100,\n",
    "           annualized_vol[highest_sharpe]*100,\n",
    "           sharpe_metric[highest_sharpe]\n",
    "          )\n",
    "         )\n",
    "        \n",
    "    #####\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    fig.set_size_inches(16, 6)\n",
    "    \n",
    "    ax.plot(sharpe_metric, \n",
    "            linewidth = 2)\n",
    "    \n",
    "    ax.set_title(\"Sharpe Ratio of Eigen-Portfolios\")\n",
    "    \n",
    "    ax.set_ylabel(\"Sharpe Ratio\")\n",
    "    \n",
    "    ax.set_xlabel(\"Portfolios\")\n",
    "    \n",
    "    #####\n",
    "        \n",
    "    results = pd.DataFrame(data = {\"Return\": annualized_ret, \"Vol\": annualized_vol, \"Sharpe\": sharpe_metric}\n",
    "                           )\n",
    "    \n",
    "    results.dropna(inplace = True)\n",
    "    \n",
    "    results.sort_values(by = [\"Sharpe\"],\n",
    "                        ascending = False,\n",
    "                        inplace = True)\n",
    "    \n",
    "    print(results.head(15)\n",
    "         )\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a43b05",
   "metadata": {},
   "source": [
    "### BACKTESTING EIGEN PORTFOLIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4789572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yet another gift\n",
    "\n",
    "def backtest_PCA_porfolios(eigen):\n",
    "\n",
    "    eigen_prtfi =\\\n",
    "        (\n",
    "            pd\n",
    "            .DataFrame(data = {\"weights\": eigen.squeeze()\n",
    "                              },\n",
    "                       index = stock_tickers)\n",
    "        )\n",
    "\n",
    "    eigen_prtfi.sort_values(by = [\"weights\"],\n",
    "                            ascending = False,\n",
    "                            inplace = True)\n",
    "\n",
    "    eigen_prtfi_returns =\\\n",
    "    (\n",
    "        np\n",
    "        .dot(X_Test_Raw\n",
    "             .loc[ : , eigen_prtfi.index],\n",
    "             eigen)\n",
    "    )\n",
    "\n",
    "    eigen_portfolio_returns =\\\n",
    "    (\n",
    "        pd\n",
    "        .Series(eigen_prtfi_returns.squeeze(),\n",
    "                index = X_Test_Raw.index)\n",
    "    )\n",
    "\n",
    "    returns, vol, sharpe = calculate_sharpe_ratio(eigen_portfolio_returns)\n",
    "\n",
    "    print(\"Our PCA-based Portfolio:\\nReturn = %.2f%%\\nVolatility = %.2f%%\\nSharpe = %.2f\"  %\n",
    "          (returns * 100, vol * 100, sharpe)\n",
    "         )\n",
    "\n",
    "    # Compared with what? Equal-weightage Portfolio\n",
    "\n",
    "    equal_weight_return =\\\n",
    "     (\n",
    "        X_Test_Raw * (1 / len(pca.components_)\n",
    "                     )\n",
    "    ).sum(axis = 1)\n",
    "\n",
    "    df_plot =\\\n",
    "        (\n",
    "            pd\n",
    "            .DataFrame({\"ML Portfolio Return\": eigen_portfolio_returns,\n",
    "                        \"Equal Weight Index\": equal_weight_return},\n",
    "                      index = X_Test.index\n",
    "                      )\n",
    "        )\n",
    "\n",
    "    (\n",
    "        np\n",
    "        .cumprod(df_plot + 1)\n",
    "        .plot(title = \"Returns of the equal weighted index vs. Eigen-Portfolio\",\n",
    "              figsize = [16, 8]\n",
    "             )\n",
    "    )\n",
    "\n",
    "    plt.show()\n",
    "backtest_PCA_porfolios(eigen = weights[1]\n",
    "                      ) # set weights, 1 is the highest strategy return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e26c14e",
   "metadata": {},
   "source": [
    "### Supervised Machine Learning on example of Microsoft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad84f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's assume that in addition to historical data on Microsoft, \n",
    "#the independent variables used are the following potentially correlated assets.\n",
    "\n",
    "stock_ticker = [\"MSFT\", \"IBM\", \"GOOGL\"]\n",
    "currency_ticker = [\"DEXJPUS\", \"DEXUSUK\"]\n",
    "index_ticker = [\"SP500\", \"DJIA\", \"VIXCLS\"]\n",
    "stock_data = pdr.get_data_yahoo(stock_ticker)\n",
    "currency_data = pdr.get_data_fred(currency_ticker)\n",
    "index_data = pdr.get_data_fred(index_ticker)\n",
    "\n",
    "# let's define outcome and predictor vars\n",
    "# they both are weekly return on msft, BUT with lag - 5,15,30, 60 for ms stocks, 5 days - for other features mentioned\n",
    "# note that trading week is 5 days\n",
    "\n",
    "return_period = 5\n",
    "Y = (np.log(stock_data.loc[ : , (\"Adj Close\", \"MSFT\")]).diff(return_period).shift(-return_period))\n",
    "Y.name = (Y.name[-1]+\"_pred\") # rename to specify our predictors\n",
    "\n",
    "# let's specify features\n",
    "X1 =(np.log(stock_data.loc[ : , (\"Adj Close\", (\"GOOGL\", \"IBM\"))]).diff(return_period)) # calculate lagged logreturns\n",
    "X1.columns = (X1.columns.droplevel()) #empty df?\n",
    "X2 = (np.log(currency_data).diff(return_period))\n",
    "X3 = (np.log(index_data).diff(return_period))\n",
    "\n",
    "# For each value i in the list [return_period, return_period * 3, return_period * 6, return_period * 12], \n",
    "# let's calculates the logarithm of the values in column (\"Adj Close\", \"MSFT\") \n",
    "# of DataFrame stock_data, \n",
    "# and then calculates the difference of consecutive log values (log-return) with lag i. \n",
    "\n",
    "# now we create different lag df for our microsoft stocks\n",
    "X4 = (pd.concat([np.log(stock_data.loc[ : , (\"Adj Close\", \"MSFT\")]).diff(i) \n",
    "                     for i in [return_period, \n",
    "                               return_period * 3, \n",
    "                               return_period * 6, \n",
    "                               return_period * 12]],axis = 1).dropna())\n",
    "X4.columns = [\"MSFT_DT\", \"MSFT_3DT\", \"MSFT_6DT\", \"MSFT_12DT\"]\n",
    "\n",
    "# now we concat all features in one df\n",
    "X = (pd.concat([X1, X2, X3, X4],axis = 1))\n",
    "\n",
    "# remember that y is our target outcome\n",
    "data = (pd.concat([Y, X],axis = 1).dropna().iloc[ : :return_period, :])\n",
    "\n",
    "# EDA - exploratory data analysis\n",
    "data.describe()\n",
    "\n",
    "# create histo\n",
    "(data.hist(bins = 35, sharex = False, sharey = False,figsize =[16, 16]))\n",
    "plt.show()\n",
    "\n",
    "# for kernel density estimation\n",
    "\n",
    "(data.plot(kind = \"density\", subplots = True,layout = (3, 4), sharex = True, legend = True, figsize = [16, 16]))\n",
    "plt.show()\n",
    "\n",
    "# create a matrix to see how much correlation there is between our outcome and different predictors\n",
    "correlation = data.corr()\n",
    "plt.figure(figsize =[16, 16])\n",
    "plt.title(\"Correlation Matrix\")\n",
    "sns.heatmap(correlation,vmax = 1, square = True, cmap = \"viridis\", annot = True)\n",
    "\n",
    "# same as above but as scatter matrix\n",
    "scatter_matrix(data, figsize = (16, 16))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a1bc9e",
   "metadata": {},
   "source": [
    "### EDA correlation matrix for DJ close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06542676",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = dow.corr()\n",
    "plt.figure(figsize = [16, 16])\n",
    "plt.title(\"A Heatmap for Correlation Matrix\")\n",
    "sns.heatmap(corr, annot = True,cmap = \"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506a846c",
   "metadata": {},
   "source": [
    "### MACHINE LEARNING WALKTHROUGH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f84300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 - data split in training and testing set\n",
    "validation_size = 0.20 #this is our testing set\n",
    "train_size = int(len(X)*(1 - validation_size))\n",
    "X_train, X_test = (X[0:train_size], X[train_size:len(X)])\n",
    "Y_train, Y_test = (Y[0:train_size], Y[train_size:len(X)])\n",
    "\n",
    "# step 4 prep - tenfold cross-validation and evaluation\n",
    "num_folds = 10\n",
    "seed = 230926\n",
    "scoring = \"neg_mean_squared_error\"\n",
    "\n",
    "# step 3 - model fitting and comparison of different models\n",
    "models = [] # create empty, then append there models\n",
    "# Regression and tree regression algorithms:\n",
    "models.append((\"LR\", LinearRegression()))\n",
    "models.append((\"LASSO\", Lasso()))\n",
    "models.append((\"EN\", ElasticNet()))\n",
    "models.append((\"CART\", DecisionTreeRegressor()))\n",
    "models.append((\"KNN\", KNeighborsRegressor()))\n",
    "models.append((\"SVR\", SVR()))\n",
    "# Ensemble models:\n",
    "models.append((\"RFR\", RandomForestRegressor())) # Bagging (Boostrap Aggregation)\n",
    "models.append((\"ETR\", ExtraTreesRegressor())) # Bagging (Boostrap Aggregation)\n",
    "models.append((\"GBR\", GradientBoostingRegressor())) # Boosting\n",
    "models.append((\"ABR\", AdaBoostRegressor())) # Boosting\n",
    "\n",
    "# !!!!! COMPARE ALGORITHMS PERFOMANCE - BEST PRACTICE IN ML !!!!!\n",
    "### Initialization of Lists:\n",
    "\n",
    "names = []\n",
    "\n",
    "kfold_results = []\n",
    "\n",
    "train_results = []\n",
    "test_results = []\n",
    "\n",
    "# Four empty lists are initialized. names will store the names of the models, \n",
    "# kfold_results will store the cross-validation results, \n",
    "# train_results and test_results will store the performance of the models on the training and testing datasets, respectively.\n",
    "\n",
    "# Looping through Models:\n",
    "# Let's iterate over a list of models. \n",
    "# Each element in the models list is a tuple containing the name of the model (name) and the model object (model).\n",
    "\n",
    "for name, model in models:\n",
    "    \n",
    "# Appending Model Names:\n",
    "# The name of the current model is appended to the names list.\n",
    "    names.append(name)\n",
    "# Let's run K-fold Cross-Validation \n",
    "    kfold = (KFold(n_splits = num_folds, random_state = seed, shuffle = True))  \n",
    "# A KFold object is created with a specified number of splits (num_folds), a random seed (seed), and shuffling enabled.\n",
    "    \n",
    "# Running Cross-Validation:\n",
    "# Let's convert MSE to positive (Here, now it becomes lower the better; See below)\n",
    "    cv_results = (-1*cross_val_score(model, X_train, Y_train, cv = kfold, scoring = scoring))\n",
    "        \n",
    "# Cross-validation is performed on the training data (X_train, Y_train) using the current model. \n",
    "# The negative mean squared error is used as the scoring metric \n",
    "# (hence multiplied by -1 to make it positive, as the convention is that higher scores are better).\n",
    "\n",
    "# Storing Cross-Validation Results:\n",
    "# The cross-validation results for the current model are appended to the kfold_results list.    \n",
    "    kfold_results.append(cv_results)\n",
    "\n",
    "# Fitting the Model on the Entire Training Set:\n",
    "    res = model.fit(X_train, Y_train) # The model is trained on the entire training dataset !!!\n",
    "\n",
    "# Evaluating Model on Training Set:\n",
    "\n",
    "# The trained model’s predictions on the training set are evaluated using the mean squared error, \n",
    "# and the result is appended to train_results.\n",
    "    train_result = mean_squared_error(res.predict(X_train), Y_train)\n",
    "    train_results.append(train_result)\n",
    "\n",
    "# Evaluating Model on Testing Set:    \n",
    "# Similarly, the model’s performance is evaluated on the testing set and appended to test_results.\n",
    "    \n",
    "    test_result = mean_squared_error(res.predict(X_test), Y_test)\n",
    "    test_results.append(test_result)\n",
    "    \n",
    "# # Printing the Results:\n",
    "\n",
    "# # The name of the model, the average cross-validation score, stdev of the cross-validation scores, \n",
    "# the training set performance, and the testing set performance are printed out.\n",
    "\n",
    "    message = \"%s: %f (%f) %f %f\" % (name, cv_results.mean(), \n",
    "                                     cv_results.std(), \n",
    "                                     train_result, \n",
    "                                     test_result)\n",
    "    print(message)\n",
    "    \n",
    "# box plot for algo comparison\n",
    "fig = plt.figure(figsize = [16, 8]\n",
    "fig.suptitle(\"Algorithm Comparison: Results of K-Fold Cross Validation\")\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(kfold_results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028c1a5a",
   "metadata": {},
   "source": [
    "### ALGO COMPARISON THE WAY IT'S DONE IN THE FIELD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70c33e8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m### ALGO COMPARISON THE WAY IT'S DONE IN THE FIELD\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure(figsize \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m8\u001b[39m])\n\u001b[0;32m      4\u001b[0m ind \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(names))\n\u001b[0;32m      5\u001b[0m width \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.30\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig = plt.figure(figsize = [16, 8])\n",
    "ind = np.arange(len(names))\n",
    "width = 0.30\n",
    "fig.suptitle(\"Comparing the Perfomance of Various Algorithms on the Training vs. Testing Data\")\n",
    "ax = fig.add_subplot(111)\n",
    "(plt.bar(ind - width/2, train_results, width = width, label = \"Errors in Training Set\"))\n",
    "# Team, this line calculates the starting x position of the bars representing \"Errors in Training Set\". \n",
    "# The width/2 term is used to shift the bars to the left, \n",
    "# so they are centered around the tick mark for each group (algorithm) on the x-axis.    \n",
    "# The bar chart will have two sets of bars for each algorithm: one for training, and onE for testing errors.     \n",
    "# By subtracting width/2 from ind, the training error bars are positioned to the left of the center of the tick marks.      \n",
    "(plt.bar(ind + width/2, test_results, width = width, label = \"Errors in Testing Set\"))\n",
    "plt.legend()\n",
    "ax.set_xticks(ind)\n",
    "ax.set_xticklabels(names)\n",
    "plt.ylabel(\"Mean Squared Error (MSE)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
